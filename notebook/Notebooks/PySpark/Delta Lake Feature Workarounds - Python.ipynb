{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Lake 0.7+ feature workarounds\r\n",
        "Azure Synapse Analytics currently runs a fork of Delta Lake 0.6.x, which does not support all SQL commands and features available in Delta Lake 0.7+. This notebook contains Python workarounds for these commands and features.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some test data.\n",
        "df = spark.sql(\"SELECT 'foo' as Col1, 'bar' as Col2\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating managed tables (with or without partitions)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS ManagedDeltaTable\")\r\n",
        "spark.sql(\"DROP TABLE IF EXISTS ExternalDeltaTable\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# CREATE TABLE tableName USING DELTA\n",
        "\n",
        "df.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "externalTablePath = \"/tutorial/delta/externaltable\"\r\n",
        "\r\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(externalTablePath)\r\n",
        "spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(externalTablePath))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# CREATE TABLE tableName USING DELTA PARTITIONED BY (...)\n",
        "\n",
        "df.write.format(\"delta\").mode(\"append\").partitionBy(\"Col1\").option(\"__partition_columns\", \"\"\"[\"Col1\"]\"\"\").saveAsTable(\"PartitionedManagedDeltaTable\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading from a storage path\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# SELECT * FROM delta.`/path/`\n",
        "\n",
        "spark.read.format(\"delta\").load(externalTablePath).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inserting from one table into another\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# INSERT INTO table1 SELECT * FROM table2\n",
        "\n",
        "spark.sql(\"SELECT * FROM ManagedDeltaTable\").write.format(\"delta\").mode(\"append\").save(externalTablePath)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# INSERT OVERWRITE table1 SELECT * FROM table2\n",
        "\n",
        "spark.sql(\"SELECT * FROM ManagedDeltaTable\").write.format(\"delta\").mode(\"overwrite\").save(externalTablePath)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating or deleting rows from a table\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# DELETE FROM tableName WHERE (...)\n",
        "\n",
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "dt = DeltaTable.forPath(spark, externalTablePath)\n",
        "\n",
        "dt.delete(\"Col1 == 'foo'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# UPDATE tableName SET (...)\n",
        "\n",
        "describeExtended = spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\")\n",
        "display(describeExtended)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the path to the table in storage.\r\n",
        "managedTablePath = describeExtended.where(\"col_name == 'Location'\").select(\"data_type\").collect()[0][0]\r\n",
        "\r\n",
        "# Construct the DeltaTable object from the path.\r\n",
        "managedTable = DeltaTable.forPath(spark, managedTablePath)\r\n",
        "\r\n",
        "# Run the update command.\r\n",
        "managedTable.update(condition = expr(\"Col1 == 'foo'\"), set = {\"Col2\": lit(\"foobar\")})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# UPDATE delta.`/path/` WHERE (...)\n",
        "\n",
        "DeltaTable.forPath(spark, externalTablePath).update(\n",
        "        condition = expr(\"Col1 == 'foo'\"),\n",
        "        set = {\"Col2\" : lit(\"foobar\")})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging two tables\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# MERGE INTO table1\n",
        "# USING table2\n",
        "# ON (...)\n",
        "# WHEN MATCHED THEN (...)\n",
        "# WHEN NOT MATCHED THEN (...)\n",
        "\n",
        "DeltaTable.forPath(spark, externalTablePath).alias(\"ExternalTable\").merge(\n",
        "  managedTable.alias(\"ManagedTable\").toDF(), \"ExternalTable.Col1 == ManagedTable.Col1\"\n",
        ").whenMatchedUpdate(\n",
        "  set = {\"Col2\" : lit(\"This row matched.\")}\n",
        ").whenNotMatchedInsert(\n",
        "  values = {\"Col2\" : lit(\"This row did not match.\")}).execute()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing the schema of a managed table.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# ALTER TABLE tableName ADD COLUMNS (...)\n",
        "# ALTER TABLE tableName CHANGE COLUMN (...)\n",
        "# ALTER TABLE tableName REPLACE COLUMNS (...)\n",
        "\n",
        "# Drop external table.\n",
        "spark.sql(\"DROP TABLE ExternalDeltaTable\")\n",
        "\n",
        "# Reconfigure the table using DataFrame APIs...\n",
        "\n",
        "# Recreate the table.\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(externalTablePath)\n",
        "spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(externalTablePath))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring table properties\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# ALTER TABLE delta.`/path`\n",
        "# SET TBLPROPERTIES(...)\n",
        "# TBLPROPERTIES(\n",
        "# delta.compatibility.symlinkFormatManifest.enabled=true)\n",
        "\n",
        "# No workaround available.\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ SQL syntax: \n",
        "# \n",
        "# TBLPROPERTIES(delta.logRetentionDuration = \"interval <interval>\")\n",
        "# TBLPROPERTIES(delta.deletedFileRetentionDuration = \"interval <interval>\")\n",
        "\n",
        "# Can only set these globally.\n",
        "spark.conf.set(\"spark.databricks.delta.properties.defaults.delta.logRetentionDuration\", \"interval 2 days\")\n",
        "spark.conf.set(\"spark.databricks.delta.properties.defaults.delta.deletedFileRetentionDuration\", \"interval 1 days\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# SET spark.databricks.delta.commitInfo.userMetadata=”{custom metadata}” INSERT …\n",
        "\n",
        "# df.write.format(\"delta\")\n",
        "#   .mode(...)\n",
        "#   .option(\"userMetadata\", \"{custom metadata}\")\n",
        "#   .save(...)\n",
        "\n",
        "# No workaround available for these.\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeltaTable.forName()\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delta Lake 0.7+ syntax: \n",
        "# \n",
        "# DeltaTable.forName(tableName)\n",
        "\n",
        "managedTablePath = spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").where(\"col_name == 'Location'\").select(\"data_type\").collect()[0][0]\n",
        "\n",
        "DeltaTable.forPath(spark, managedTablePath)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}