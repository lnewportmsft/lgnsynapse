{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started with Azure Cosmos DB's API for MongoDB and Synapse Link\n",
        "\n",
        "## Key Information about this notebook\n",
        "\n",
        "* This notebook is part of the Azure Synapse Link for Azure Cosmos DB analitycal sample notebooks. For more information, click [here](../../../README.md). \n",
        "\n",
        "* It was build for Azure Cosmos DB API for MongoDB but you can, by yourself, customize it for Azure Cosmos DB SQL API. Please read about the analytical store inference schema differences between these 2 APIs [here](https://docs.microsoft.com/azure/cosmos-db/analytical-store-introduction#analytical-schema). \n",
        "\n",
        "* This is a Synapse Notebook and it was created to run in Synapse Analytics workspaces. Please make sure that you followed the pre-reqs of the [README](/README.md) file. After that, please execute the steps below in the same order that they are presented here. \n",
        "\n",
        "* From now on, all operations are case sentitive. Please be careful with everything you need to type.\n",
        "\n",
        "In this sample we will execute the following tasks:\n",
        "\n",
        "1. Insert a dataset using the traditional MongoDB client.\n",
        "1. Execute aggregation queries against the Analytical Store from the transactional data we inserted.\n",
        "1. Insert another dataset, but this time using a different datatype for the timestamp property.\n",
        "1. Execute aggregation queries again, consolidating both datasets.\n",
        "\n",
        "## Pre-requisites\n",
        "1. Have you created a MongoDB API account in Azure Cosmos DB? If not, go to [Create an account for Azure Cosmos DB's API for MongoDB](https://docs.microsoft.com/azure/cosmos-db/mongodb-introduction).\n",
        "1. For your Cosmos DB account, have you enabled Synapse Link? If not, go to [Enable Synapse Link for Azure Cosmos DB's API for MongoDB](https://docs.microsoft.com/azure/cosmos-db/configure-synapse-link).\n",
        "1. Have you created a Synapse Workspace? If not, go to [Create Synapse Workspace account](https://docs.microsoft.com/azure/synapse-analytics/synapse-link/how-to-connect-synapse-link-cosmos-db). Please don't forget to add yourself as **Storage Blob Data Contributor** to the primary ADLS G2 account that is linked to the Synapse workspace.\n",
        "\n",
        "## Create a Cosmos DB collection with analytical store enabled\n",
        "\n",
        "Please be careful, all commands are case sensitive.\n",
        "\n",
        "1. Create a database named `DemoSynapseLinkMongoDB`. \n",
        "1. Create a collection named `HTAP` with a Shard key called `item`. Make sure you set the `Analytical store` option to `On` when you create your collection.\n",
        "\n",
        "## Optional - Connect your collection to Synapse\n",
        "\n",
        "To accelerate future work, you can connect your collection to Synapse. **We won't use this capability in this demo**, but fell free to test and use it.\n",
        "\n",
        "1. Go to your Synapse Analytics workspace.\n",
        "1. Create a `Linked Data` connection for your MongoDB API account. \n",
        "    1. Under the `Data` blade, select the + (plus) sign.\n",
        "    1. Select the `Connect to external data` option.\n",
        "    1. Now select the `Azure Cosmos DB (MongoDB API)` option. \n",
        "    1. Enter all the information regarding your specific Azure Cosmos DB account either by using the dropdowns or by entering the connection string. Take note of the name you assigned to your `Linked Data` connection. \n",
        "    - Alternatively, you can also use the connection parameters from your account overview.\n",
        "1. Test the connection by looking for your database accounts in the `Data` blade, and under the `Linked` tab.\n",
        "    - There should be a list that contains all accounts and collections.\n",
        "    - Collections that have an `Analytical Store` enabled will have a distinctive icon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's get the environment ready\n",
        "\n",
        "This environment allows you to install and use any python libraries that you want to run. For this sample, you need to add the following libraries to your Spark pool:\n",
        "\n",
        "```\n",
        "pymongo==3.5.1\n",
        "aenum==2.2.4\n",
        "```\n",
        "\n",
        "Learn how to import libraries into your Spark pools in [this article](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-portal-add-libraries). Please use the `requirements.txt` file located in the same folder of this notebook to update your pool packages.\n",
        "\n",
        "You can execute the following command to make sure all the libraries are installed correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:33:29.2530674Z",
              "execution_start_time": "2020-10-30T01:36:36.0568472Z",
              "execution_finish_time": "2020-10-30T01:36:38.0833246Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "pymongo OK\nbson OK\naenum OK"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "import importlib\r\n",
        "\r\n",
        "packages = ['pymongo','aenum']\r\n",
        "for package in packages:\r\n",
        "    test = importlib.util.find_spec(package)\r\n",
        "    if test:\r\n",
        "        print(package, \"OK\")\r\n",
        "    else:\r\n",
        "        print(package, \"PROBLEM - NOK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add your database account and collection details here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 5,
              "state": "submitted",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:36:50.8652665Z",
              "execution_start_time": "2020-10-30T01:36:50.8989056Z",
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 5, Submitted, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "DATABASE_ACCOUNT_NAME = 'your-cosmos-db-mongodb-account-name'\n",
        "DATABASE_ACCOUNT_READWRITE_KEY = 'your-cosmos-db-mongodb-account-key'\n",
        "DATABASE_NAME = 'DemoSynapseLinkMongoDB'\n",
        "COLLECTION_NAME = 'HTAP'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's initialize the MongoDB client\n",
        "\n",
        "You are only going to need the following parameters from your account overview: \n",
        "- Connection string.\n",
        "- Primary or secondary ready/write key.\n",
        "\n",
        "Remember that we named our database `DemoSynapseLinkMongoDB` and our collection `HTAP`.\n",
        "\n",
        "The code snippet below shows how to initialize the `MongoClient` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:39:06.642964Z",
              "execution_start_time": "2020-10-30T01:39:06.6716214Z",
              "execution_finish_time": "2020-10-30T01:39:08.7084153Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pymongo import MongoClient\n",
        "from bson import ObjectId # For ObjectId to work\n",
        "\n",
        "client = MongoClient(\"mongodb://{account}.mongo.cosmos.azure.com:10255/?ssl=true&replicaSet=globaldb\".format(account = DATABASE_ACCOUNT_NAME)) # Your own database account endpoint.\n",
        "db = client.DemoSynapseLinkMongoDB    # Select the database\n",
        "db.authenticate(name=DATABASE_ACCOUNT_NAME,password=DATABASE_ACCOUNT_READWRITE_KEY) # Use your database account name and any of your read/write keys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inserting data with the MongoClient driver\n",
        "\n",
        "The following sample will generate 500 items based on random data. Each item will contain the following fields:\n",
        "- item, string\n",
        "- price, float\n",
        "- rating, integer\n",
        "- timestamp, [epoch integer](http://unixtimestamp.50x.eu/about.php)\n",
        "\n",
        "This data will be inserted into the MongoDB store of your database. This emulates the transactional data that an application would generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:39:12.4535574Z",
              "execution_start_time": "2020-10-30T01:39:12.4801232Z",
              "execution_finish_time": "2020-10-30T01:39:28.7334713Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "finished creating 500 orders"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from random import randint\n",
        "import time\n",
        "\n",
        "orders = db[\"HTAP\"]\n",
        "\n",
        "items = ['Pizza','Sandwich','Soup', 'Salad', 'Tacos']\n",
        "prices = [2.99, 3.49, 5.49, 12.99, 54.49]\n",
        "\n",
        "for x in range(1, 501):\n",
        "    order = {\n",
        "        'item' : items[randint(0, (len(items)-1))],\n",
        "        'price' : prices[randint(0, (len(prices)-1))],\n",
        "        'rating' : randint(1, 5),\n",
        "        'timestamp' : time.time()\n",
        "    }\n",
        "    \n",
        "    result=orders.insert_one(order)\n",
        "\n",
        "print('finished creating 500 orders')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read data from the Analytical Store.\n",
        "\n",
        "Now that we have inserted some transactional data, let's read it from Azure Cosmos DB analytical store. Cosmos DB will automatically transform the BSON data (Binary JSON) into a columnar format, which will make it fast and easy to execute aggregation workloads on top of your transactional data, at no RUs or performance costs.\n",
        "\n",
        "The cells below will:\n",
        "\n",
        "1. Load the data from analytical store into a DataFrame.\n",
        "1. Check the top rows. Yes, the BSON data was converted into columar structured format.\n",
        "1. Check the DataFrame schema.\n",
        "1. Run aggregations\n",
        "\n",
        "\n",
        "> If you get an \"no snapshot\" error, Please check if your container was created with **analytical store** enabled. \n",
        "\n",
        "If your DataFrame has no data, please wait a couple of minutes because the root cause is that the auto sync between transactional and analytical stores isn't completed yet. This process usually takes 2 minutes, but in some cases it may take up to 5 minutes. Please wait a few minutes and run the command below again.\n",
        "\n",
        "**Important: Please note that we are using random values for prices and ratings. Don't expect the same results of the outputs below. What you can expect is the same behavior and experience.**\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:41:08.3750827Z",
              "execution_start_time": "2020-10-30T01:41:08.4053202Z",
              "execution_finish_time": "2020-10-30T01:41:22.6887631Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "409d5e1d-b7a4-42f6-99f9-a860e6410a05",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 409d5e1d-b7a4-42f6-99f9-a860e6410a05)"
          },
          "metadata": {}
        },
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {},
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Load the data from analytical store into a DataFrame.\n",
        "df = spark.read.format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.cosmos.accountEndpoint\", \"https://{account}.documents.azure.com:443/\".format(account = DATABASE_ACCOUNT_NAME))\\\n",
        "    .option(\"spark.cosmos.accountKey\", DATABASE_ACCOUNT_READWRITE_KEY)\\\n",
        "    .option(\"spark.cosmos.database\", DATABASE_NAME)\\\n",
        "    .option(\"spark.cosmos.container\", COLLECTION_NAME)\\\n",
        "    .load()\n",
        "\n",
        "# Checking the data\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## First Schema Analysis\r\n",
        "\r\n",
        "Let's run the command below and check the schema of the `df` DataFrame that we just created and loaded. Please note that all properties of our document (item, price, rating, and timestamp) are represented in the DataFrame as a `struct` with one datatype within each one of them. This will change in the next cells, and to understand that is part of the learning objectives of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:41:29.5737507Z",
              "execution_start_time": "2020-10-30T01:41:29.5986476Z",
              "execution_finish_time": "2020-10-30T01:41:31.632259Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "root\n |-- _rid: string (nullable = true)\n |-- _ts: long (nullable = true)\n |-- id: string (nullable = true)\n |-- _etag: string (nullable = true)\n |-- _id: struct (nullable = true)\n |    |-- objectId: string (nullable = true)\n |-- item: struct (nullable = true)\n |    |-- string: string (nullable = true)\n |-- price: struct (nullable = true)\n |    |-- float64: double (nullable = true)\n |-- rating: struct (nullable = true)\n |    |-- int32: integer (nullable = true)\n |-- timestamp: struct (nullable = true)\n |    |-- float64: double (nullable = true)\n |-- _partitionKey: struct (nullable = true)\n |    |-- string: string (nullable = true)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Agregations\r\n",
        "\r\n",
        "Now let's run aggregations on top of the `df` DataFrame that we just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:41:36.6932504Z",
              "execution_start_time": "2020-10-30T01:41:36.7200694Z",
              "execution_finish_time": "2020-10-30T01:41:38.7556946Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+------------+------------+\n|item[string]|    sum(_ts)|\n+------------+------------+\n|        Soup|147570020190|\n|       Pizza|166818283753|\n|       Salad|150778064041|\n|       Tacos|174838393606|\n|    Sandwich|162006217845|\n+------------+------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Run aggregations\r\n",
        "\r\n",
        "df.groupBy(df.item.string).sum().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Important - Spark Syntax for Aggregations on DataFrames\r\n",
        "\r\n",
        "For the aggregation above, a syntax that doesn't explicity mentions the datatypes, like `df.groupBy(df['item']).sum().show()`, executes without an error. But it is **not recommended!** \r\n",
        "\r\n",
        "It runs because Spark automatically flattens the structure into an Array, where it takes each distinct value in the `struct` dict and applies the aggregation function. But you will see in the next cells that we may have more than one datatype for the same struct of a property, and the implicit conversion that Spark does can cause wrong results.\r\n",
        "## Schema Representation - A quick note about the MongoDB schema in analytical store\r\n",
        "\r\n",
        "Please note in the result above that for the `timestamp` field we have only 1 datatype: `struct<float64:double>`. \r\n",
        "We will see that this detail will change since we will insert data with different datatype for that `timestamp` field.\r\n",
        "\r\n",
        "For Azure Cosmos DB API for MongoDB accounts, we make use of a **Full Fidelity Schema** as a default option. This is a representation of property names extended with their data types to provide an accurate \r\n",
        "representation of their values and avoid ambiguity.\r\n",
        "\r\n",
        "This is why, when we called the fields above, we used their datatype as a suffix. Like in the example below:\r\n",
        "\r\n",
        "```\r\n",
        "df.filter((df.item.string == \"Pizza\")).show(10)\r\n",
        "```\r\n",
        "\r\n",
        "Notice how we specified the `string` type after the name of the property. Here is a map of all potential properties and their suffix representations in the Analytical Store:\r\n",
        "\r\n",
        "| Original Data Type     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Suffix    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Example &nbsp;&nbsp;&nbsp;&nbsp; | \r\n",
        "|---------------|----------------|--------|\r\n",
        "| Double        | \".float64\"     |  `24.99`   |\r\n",
        "| Array         | \".array\"       |  `[\"a\", \"b\"]`   |\r\n",
        "| Binary        | \".binary\"      |  `0`   |\r\n",
        "| Boolean       | \".bool\"        |  `True`   |\r\n",
        "| Int32         | \".int32\"       |  `123`   |\r\n",
        "| Int64         | \".int64\"       |  `255486129307`   |\r\n",
        "| Null          | \".null\"        |  `null`   |\r\n",
        "| String        | \".string\"      |  `\"ABC\"`   |\r\n",
        "| Timestamp     | \".timestamp\"   |  `Timestamp(0, 0)`   |\r\n",
        "| DateTime      | \".date\"        |  `ISODate(\"2020-08-21T07:43:07.375Z\")`   |\r\n",
        "| ObjectId      | \".objectId\"    |  `ObjectId(\"5f3f7b59330ec25c132623a2\")`   |\r\n",
        "| Document      | \".object\"      |  `{\"a\": \"a\"}`   |\r\n",
        "\r\n",
        "These types are inferred from the data that is inserted in the transactional store. You can see the schema by executing the following command:\r\n",
        "```\r\n",
        "df.printSchema\r\n",
        "```\r\n",
        "\r\n",
        "> The default option for Azure Cosmos DB CORE (SQL) API, **Well defined Schema** is the default option. For more information about schemas representation, click [here](https://docs.microsoft.com/azure/cosmos-db/analytical-store-introduction#schema-representation) .\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's insert more orders!\n",
        "\n",
        "This time we will use slightly different data. Each item will contain the following fields:\n",
        "- item, string\n",
        "- price, float\n",
        "- rating, integer\n",
        "- timestamp, [ISO String format](https://en.wikipedia.org/wiki/ISO_8601)\n",
        "\n",
        "Notice how the `Timestamp` field is now in a string format. This will help us understand how the different data fields can be read based on their data type.\n",
        "\n",
        "After that, we will load the data, check the schema, and run some queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:42:56.3886624Z",
              "execution_start_time": "2020-10-30T01:42:56.4190522Z",
              "execution_finish_time": "2020-10-30T01:43:12.6787259Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "finished creating 500 orders"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from random import randint\n",
        "from time import strftime\n",
        "\n",
        "orders = db[\"HTAP\"]\n",
        "\n",
        "items = ['Pizza','Sandwich','Soup', 'Salad', 'Tacos']\n",
        "prices = [2.99, 3.49, 5.49, 12.99, 54.49]\n",
        "\n",
        "for x in range(1, 501):\n",
        "    order = {\n",
        "        'item' : items[randint(0, (len(items)-1))],\n",
        "        'price' : prices[randint(0, (len(prices)-1))],\n",
        "        'rating' : randint(1, 5),\n",
        "        'timestamp' : strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "    \n",
        "    result=orders.insert_one(order)\n",
        "\n",
        "print('finished creating 500 orders')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's reload the DataFrame and check the schema again!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 20,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:44:01.6061324Z",
              "execution_start_time": "2020-10-30T01:44:01.6344273Z",
              "execution_finish_time": "2020-10-30T01:44:03.6574889Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 20, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "root\n |-- _rid: string (nullable = true)\n |-- _ts: long (nullable = true)\n |-- id: string (nullable = true)\n |-- _etag: string (nullable = true)\n |-- _id: struct (nullable = true)\n |    |-- objectId: string (nullable = true)\n |-- item: struct (nullable = true)\n |    |-- string: string (nullable = true)\n |-- price: struct (nullable = true)\n |    |-- float64: double (nullable = true)\n |-- rating: struct (nullable = true)\n |    |-- int32: integer (nullable = true)\n |-- timestamp: struct (nullable = true)\n |    |-- float64: double (nullable = true)\n |    |-- string: string (nullable = true)\n |-- _partitionKey: struct (nullable = true)\n |    |-- string: string (nullable = true)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Load the Analytical Store data into a dataframe\n",
        "# Make sure to run the cell with the secrets to get the DATABASE_ACCOUNT_NAME and the DATABASE_ACCOUNT_READWRITE_KEY variables.\n",
        "df = spark.read.format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.cosmos.accountEndpoint\", \"https://{account}.documents.azure.com:443/\".format(account = DATABASE_ACCOUNT_NAME))\\\n",
        "    .option(\"spark.cosmos.accountKey\", DATABASE_ACCOUNT_READWRITE_KEY)\\\n",
        "    .option(\"spark.cosmos.database\", DATABASE_NAME)\\\n",
        "    .option(\"spark.cosmos.container\", COLLECTION_NAME)\\\n",
        "    .load()\n",
        "\n",
        "# Check the schema AGAIN. Try to find something different.\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Schema Representation - What Changed?\r\n",
        "\r\n",
        "Please note in the result above that now, for the `timestamp` field, we have 2 datatypes: `struct<float64:double>` and `string:string`. That happened because we added data with a different datatype. That's `Full Fidelity Schema`, when Azure Cosmos DB will do a full representation of your data, with the datatypes you used.\r\n",
        "\r\n",
        "> If the result doesn't show two datatypes for `timestamp`, then wait a few minutes because the backend auto-sync process has not yet occurred.\r\n",
        "\r\n",
        "## Queries\r\n",
        "\r\n",
        "Now let's run some interesting queries, using the datypes to filter the data.\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:44:38.7752449Z",
              "execution_start_time": "2020-10-30T01:44:38.8057393Z",
              "execution_finish_time": "2020-10-30T01:44:40.8597917Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-------------------------------+--------+\n|sum(price.float64 AS `float64`)|count(1)|\n+-------------------------------+--------+\n|             1736.4900000000007|     101|\n+-------------------------------+--------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# SQL!!\r\n",
        "# Let's see the data for pizzas that have a string timestamp\r\n",
        "df.createOrReplaceTempView(\"Pizza\")\r\n",
        "sql_results = spark.sql(\"SELECT sum(price.float64),count(*) FROM Pizza where timestamp.string is not null and item.string = 'Pizza'\")\r\n",
        "sql_results.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 22,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:44:28.8293425Z",
              "execution_start_time": "2020-10-30T01:44:28.8682483Z",
              "execution_finish_time": "2020-10-30T01:44:30.8886741Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 22, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-------------------------------+--------+\n|sum(price.float64 AS `float64`)|count(1)|\n+-------------------------------+--------+\n|             1571.9600000000007|     104|\n+-------------------------------+--------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# SQL!!\r\n",
        "# Let's see the data for pizzas that have a string timestamp\r\n",
        "df.createOrReplaceTempView(\"Pizza\")\r\n",
        "sql_results = spark.sql(\"SELECT sum(price.float64),count(*) FROM Pizza where timestamp.float64 is not null and item.string = 'Pizza'\")\r\n",
        "sql_results.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 34,
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-30T01:44:33.7501893Z",
              "execution_start_time": "2020-10-30T01:44:33.7809776Z",
              "execution_finish_time": "2020-10-30T01:44:35.8165643Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 34, 23, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-----------------------------------+---------------------------------+\n|max(timestamp.float64 AS `float64`)|max(timestamp.string AS `string`)|\n+-----------------------------------+---------------------------------+\n|               1.6040219670579042E9|              2020-10-30 01:43:10|\n+-----------------------------------+---------------------------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# SQL!!\r\n",
        "# Let's compare both timestamp columns\r\n",
        "df.createOrReplaceTempView(\"Pizza\")\r\n",
        "sql_results = spark.sql(\"SELECT max(timestamp.float64),max(timestamp.string) FROM Pizza where item.string = 'Pizza'\")\r\n",
        "sql_results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Schema Representation - Last thoughts\r\n",
        "\r\n",
        "Please note that the queries above return different data because of the filters on the timestamp column. From the user perspective, it's like there are 2 different columns, `timestamp.float64` and `timestamp.string`.\r\n",
        "\r\n",
        "## Conclusion\r\n",
        "\r\n",
        "Now you know how to use Azure Synapse Link for Azure Cosmos DB analitical store for MongoDB API. Also, now you know how to work with dataframes, full fidelity schema, and Spark Sql."
      ]
    }
  ]
}