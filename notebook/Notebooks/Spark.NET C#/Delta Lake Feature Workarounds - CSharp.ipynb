{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Lake 0.7+ feature workarounds\r\n",
        "Azure Synapse Analytics currently runs a fork of Delta Lake 0.6.x, which does not support all SQL commands and features available in Delta Lake 0.7+. This notebook contains .NET workarounds for these commands and features.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Generate some test data.\n",
        "var df = spark.Sql(\"SELECT 'foo' as Col1, 'bar' as Col2\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating managed tables (with or without partitions)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "spark.Sql(\"DROP TABLE IF EXISTS ManagedDeltaTable\");\r\n",
        "spark.Sql(\"DROP TABLE IF EXISTS ExternalDeltaTable\");\r\n",
        "spark.Sql(\"DROP TABLE IF EXISTS PartitionedManagedDeltaTable\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// CREATE TABLE tableName USING DELTA\n",
        "\n",
        "df.Write().\n",
        "    Format(\"delta\").\n",
        "    SaveAsTable(\"ManagedDeltaTable\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "var externalTablePath = \"/tutorial/delta/externaltable\";\r\n",
        "df.Write().\r\n",
        "    Format(\"delta\").\r\n",
        "    Mode(\"overwrite\").\r\n",
        "    Save(externalTablePath);\r\n",
        "spark.Sql($\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{externalTablePath}'\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// CREATE TABLE tableName USING DELTA PARTITIONED BY (...)\n",
        "\n",
        "df.Write().\n",
        "  Format(\"delta\").\n",
        "  Mode(\"append\").\n",
        "  PartitionBy(\"Col1\").\n",
        "  Option(\"__partition_columns\", \"[\\\"Col1\\\"]\").\n",
        "  SaveAsTable(\"PartitionedManagedDeltaTable\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading from a storage path\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// SELECT * FROM delta.`/path/`\n",
        "\n",
        "spark.Read().\n",
        "    Format(\"delta\").\n",
        "    Load(externalTablePath).\n",
        "    Show();"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inserting from one table into another\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// INSERT INTO table1 SELECT * FROM table2\n",
        "\n",
        "spark.Sql(\"SELECT * FROM ManagedDeltaTable\").\n",
        "    Write().\n",
        "    Format(\"delta\").\n",
        "    Mode(\"append\").\n",
        "    Save(externalTablePath);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// INSERT OVERWRITE table1 SELECT * FROM table2\n",
        "\n",
        "spark.Sql(\"SELECT * FROM ManagedDeltaTable\").\n",
        "    Write().\n",
        "    Format(\"delta\").\n",
        "    Mode(\"overwrite\").\n",
        "    Save(externalTablePath);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating or deleting rows from a table\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// DELETE FROM tableName WHERE (...)\n",
        "\n",
        "using Microsoft.Spark.Extensions.Delta;\n",
        "using Microsoft.Spark.Extensions.Delta.Tables;\n",
        "using Microsoft.Spark.Sql;\n",
        "using static Microsoft.Spark.Sql.Functions;\n",
        "\n",
        "var dt = DeltaTable.ForPath(externalTablePath);\n",
        "\n",
        "dt.Delete(\"Col1 == 'foo'\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// UPDATE tableName SET (...)\n",
        "\n",
        "var describeExtended = spark.Sql(\"DESCRIBE EXTENDED ManagedDeltaTable\");\n",
        "display(describeExtended);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Get the path to the table in storage.\r\n",
        "var managedTablePath = (string)describeExtended.\r\n",
        "    Where(\"col_name == 'Location'\").\r\n",
        "    Select(\"data_type\").\r\n",
        "    Collect().\r\n",
        "    First().\r\n",
        "    Get(0);\r\n",
        "\r\n",
        "// Construct the DeltaTable object from the path.\r\n",
        "var managedTable = DeltaTable.ForPath(managedTablePath);\r\n",
        "\r\n",
        "// Run the update command.\r\n",
        "managedTable.Update(\r\n",
        "        condition: Expr(\"Col1 == 'foo'\"),\r\n",
        "        set: new Dictionary<string, Column>(){{ \"Col2\", Lit(\"foobar\") }});"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// UPDATE delta.`/path/` WHERE (...)\n",
        "DeltaTable.ForPath(externalTablePath).\n",
        "    Update(\n",
        "        condition: Expr(\"Col1 == 'foo'\"),\n",
        "        set: new Dictionary<string, Column>(){{ \"Col2\", Lit(\"foobar\") }});"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging two tables\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// MERGE INTO table1\n",
        "// USING table2\n",
        "// ON (...)\n",
        "// WHEN MATCHED THEN (...)\n",
        "// WHEN NOT MATCHED THEN (...)\n",
        "\n",
        "DeltaTable.ForPath(externalTablePath).As(\"ExternalTable\").\n",
        "  Merge(managedTable.As(\"ManagedTable\").ToDF(), \"ExternalTable.Col1 == ManagedTable.Col1\").\n",
        "  WhenMatched().\n",
        "    Update(new Dictionary<string, Column>(){{ \"Col2\", Lit(\"This row matched\") }}).\n",
        "  WhenNotMatched().\n",
        "    Insert(new Dictionary<string, Column>(){{ \"Col2\", Lit(\"This row did not match\") }}).\n",
        "  Execute();\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing the schema of a managed table.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// ALTER TABLE tableName ADD COLUMNS (...)\n",
        "// ALTER TABLE tableName CHANGE COLUMN (...)\n",
        "// ALTER TABLE tableName REPLACE COLUMNS (...)\n",
        "\n",
        "// Drop external table.\n",
        "spark.Sql(\"DROP TABLE ExternalDeltaTable\");\n",
        "\n",
        "// Reconfigure the table using DataFrame APIs...\n",
        "\n",
        "// Recreate the table.\n",
        "df.Write().\n",
        "    Format(\"delta\").\n",
        "    Mode(\"overwrite\").\n",
        "    Save(externalTablePath);\n",
        "spark.Sql($\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{externalTablePath}'\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring table properties\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// ALTER TABLE delta.`/path`\n",
        "// SET TBLPROPERTIES(...)\n",
        "// TBLPROPERTIES(\n",
        "// delta.compatibility.symlinkFormatManifest.enabled=true)\n",
        "\n",
        "// No workaround available.\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// TBLPROPERTIES(delta.logRetentionDuration = \"interval <interval>\")\n",
        "// TBLPROPERTIES(delta.deletedFileRetentionDuration = \"interval <interval>\")\n",
        "\n",
        "// Can only set these globally.\n",
        "spark.Conf().Set(\"spark.databricks.delta.properties.defaults.delta.logRetentionDuration\", \"interval 2 days\");\n",
        "spark.Conf().Set(\"spark.databricks.delta.properties.defaults.delta.deletedFileRetentionDuration\", \"interval 1 days\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// SET spark.databricks.delta.commitInfo.userMetadata=”{custom metadata}” INSERT …\n",
        "\n",
        "// df.write.format(\"delta\")\n",
        "//   .mode(...)\n",
        "//   .option(\"userMetadata\", \"{custom metadata}\")\n",
        "//   .save(...)\n",
        "\n",
        "// No workaround available for these.\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeltaTable.forName()\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Delta Lake 0.7+ syntax: \n",
        "// \n",
        "// DeltaTable.forName(tableName)\n",
        "\n",
        "var managedTablePath = (string)describeExtended.\n",
        "    Where(\"col_name == 'Location'\").\n",
        "    Select(\"data_type\").\n",
        "    Collect().\n",
        "    First().\n",
        "    Get(0);\n",
        "\n",
        "DeltaTable.ForPath(managedTablePath);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_sparkdotnet",
      "display_name": "csharp"
    },
    "language_info": {
      "name": "csharp"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}