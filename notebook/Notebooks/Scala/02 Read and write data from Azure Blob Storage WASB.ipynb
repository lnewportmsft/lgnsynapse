{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
        "\n",
        "You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
        "\n",
        "    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
        "\n",
        "This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a sample data\n",
        "\n",
        "Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets as a sample."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// set blob storage account connection for open dataset\n",
        "\n",
        "val hol_blob_account_name = \"azureopendatastorage\"\n",
        "val hol_blob_container_name = \"holidaydatacontainer\"\n",
        "val hol_blob_relative_path = \"Processed\"\n",
        "val hol_blob_sas_token = \"\"\n",
        "\n",
        "val hol_wasbs_path = f\"wasbs://$hol_blob_container_name@$hol_blob_account_name.blob.core.windows.net/$hol_blob_relative_path\"\n",
        "spark.conf.set(f\"fs.azure.sas.$hol_blob_container_name.$hol_blob_account_name.blob.core.windows.net\",hol_blob_sas_token)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "hol_blob_account_name: String = azureopendatastorage\nhol_blob_container_name: String = holidaydatacontainer\nhol_blob_relative_path: String = Processed\nhol_blob_sas_token: String = \"\"\nhol_wasbs_path: String = wasbs://holidaydatacontainer@azureopendatastorage.blob.core.windows.net/Processed"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// load the sample data as a Spark DataFrame\n",
        "val hol_df = spark.read.parquet(hol_wasbs_path) \n",
        "hol_df.show(5, truncate = false)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "hol_df: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 4 more fields]\n+---------------+--------------------------+--------------------------+-------------+-----------------+-------------------+\n|countryOrRegion|holidayName               |normalizeHolidayName      |isPaidTimeOff|countryRegionCode|date               |\n+---------------+--------------------------+--------------------------+-------------+-----------------+-------------------+\n|Argentina      |Año Nuevo [New Year's Day]|Año Nuevo [New Year's Day]|null         |AR               |1970-01-01 00:00:00|\n|Australia      |New Year's Day            |New Year's Day            |null         |AU               |1970-01-01 00:00:00|\n|Austria        |Neujahr                   |Neujahr                   |null         |AT               |1970-01-01 00:00:00|\n|Belgium        |Nieuwjaarsdag             |Nieuwjaarsdag             |null         |BE               |1970-01-01 00:00:00|\n|Brazil         |Ano novo                  |Ano novo                  |null         |BR               |1970-01-01 00:00:00|\n+---------------+--------------------------+--------------------------+-------------+-----------------+-------------------+\nonly showing top 5 rows"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write data to Azure Storage Blob\n",
        "\n",
        "Synapse leverage **Shared access signature (SAS)** to access Azure Blob Storage. To avoid exposing SAS keys in the code, we recommend creating a new linked service in Synapse workspace to the Azure Blob Storage account you want to access.\n",
        "\n",
        "Follow these steps to add a new linked service for an Azure Blob Storage account:\n",
        "\n",
        "1. Open the [Azure Synapse Studio](https://web.azuresynapse.net/).\n",
        "2. Select **Manage** from the left panel and select **Linked services** under the **External connections**.\n",
        "3. Search **Azure Blob Storage** in the **New linked Service** panel on the right.\n",
        "4. Select **Continue**.\n",
        "5. Select the Azure Blob Storage Account to access and configure the linked service name. Suggest using **Account key** for the **Authentication method**.\n",
        "6. Select **Test connection** to validate the settings are correct.\n",
        "7. Select **Create** first and click **Publish all** to save your changes.\n",
        "\n",
        "You can access data on Azure Blob Storage with Synapse Spark via following URL:\n",
        "\n",
        "```wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/```\n",
        "\n",
        "Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// set your blob storage account connection\n",
        "\n",
        "val blob_account_name = \"\" // replace with your blob name\n",
        "val blob_container_name = \"\" //replace with your container name\n",
        "val blob_relative_path = \"\" //replace with your relative folder path\n",
        "val linked_service_name = \"\" //replace with your linked service name\n",
        "\n",
        "val blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "\n",
        "val wasbs_path = f\"wasbs://$blob_container_name@$blob_account_name.blob.core.windows.net/$blob_relative_path\"\n",
        "spark.conf.set(f\"fs.azure.sas.$blob_container_name.$blob_account_name.blob.core.windows.net\",blob_sas_token)\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "blob_account_name: String = samplenbblob\nblob_container_name: String = data\nblob_relative_path: String = samplenb/\nblob_sas_token: String = ?sv=2019-02-02&ss=b&srt=sco&sp=rwdlac&se=2021-03-23T17:05:16Z&st=2020-03-24T09:05:16Z&spr=https,http&sig=drtIrL68s07nPW0Q9WEb5XFL6y5Eb7%2BOpmpxGyAHLaw%3D\nwasbs_path: String = wasbs://data@samplenbblob.blob.core.windows.net/samplenb/"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save a dataframe as Parquet, JSON or CSV\n",
        "If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
        "\n",
        "Dataframes can be saved in any format, regardless of the input format.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// set the path for the output file\n",
        "\n",
        "val parquet_path = wasbs_path + \"holiday.parquet\"\n",
        "val json_path = wasbs_path + \"holiday.json\"\n",
        "val csv_path = wasbs_path + \"holiday.csv\""
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "parquet_path: String = wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.parquet\njson_path: String = wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.json\ncsv_path: String = wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.csv"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.SaveMode\n",
        "\n",
        "hol_df.write.mode(SaveMode.Overwrite).parquet(parquet_path)\n",
        "hol_df.write.mode(SaveMode.Overwrite).json(json_path)\n",
        "hol_df.write.mode(SaveMode.Overwrite).option(\"header\", \"true\").csv(csv_path)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "import org.apache.spark.sql.SaveMode"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save a dataframe as text files\n",
        "If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Define the text file path and covert spark dataframe into RDD\n",
        "val text_path = wasbs_path + \"holiday.txt\"\n",
        "val hol_RDD = hol_df.rdd"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "text_path: String = wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.txt\nhol_RDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[18] at rdd at <console>:30"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have an RDD, you can convert it to a text file like the following:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Save RDD as text file\n",
        "hol_RDD.saveAsTextFile(text_path)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data from Azure Storage Blob\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataframe from parquet files\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val df_parquet = spark.read.parquet(parquet_path)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "df_parquet: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 4 more fields]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataframe from JSON files\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val df_json = spark.read.json(json_path)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "df_json: org.apache.spark.sql.DataFrame = [countryOrRegion: string, countryRegionCode: string ... 4 more fields]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataframe from CSV files\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val df_csv = spark.read.option(\"header\", \"true\").csv(csv_path)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "df_csv: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 4 more fields]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an RDD from text file\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val text = sc.textFile(text_path)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "text: org.apache.spark.rdd.RDD[String] = wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.txt MapPartitionsRDD[36] at textFile at <console>:32"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "text.take(5).foreach(println)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "[Argentina,Año Nuevo [New Year's Day],Año Nuevo [New Year's Day],null,AR,1970-01-01 00:00:00.0]\n[Australia,New Year's Day,New Year's Day,null,AU,1970-01-01 00:00:00.0]\n[Austria,Neujahr,Neujahr,null,AT,1970-01-01 00:00:00.0]\n[Belgium,Nieuwjaarsdag,Nieuwjaarsdag,null,BE,1970-01-01 00:00:00.0]\n[Brazil,Ano novo,Ano novo,null,BR,1970-01-01 00:00:00.0]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "scala"
    },
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "microsoft": {
      "language": "scala"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}