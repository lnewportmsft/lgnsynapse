{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delta Lake 0.7+ feature workarounds\n",
        "Azure Synapse Analytics currently runs a fork of Delta Lake 0.6.x, which does not support all SQL commands and features available in Delta Lake 0.7+. This notebook contains Scala workarounds for these commands and features.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Generate some test data.\n",
        "val df = spark.sql(\"SELECT 'foo' as Col1, 'bar' as Col2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating managed tables (with or without partitions)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS ManagedDeltaTable\")\r\n",
        "spark.sql(\"DROP TABLE IF EXISTS ExternalDeltaTable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// CREATE TABLE tableName USING DELTA\n",
        "\n",
        "df.write.\n",
        "    format(\"delta\").\n",
        "    saveAsTable(\"ManagedDeltaTable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "val externalTablePath = \"/tutorial/delta/externaltable\"\r\n",
        "df.write.\r\n",
        "    format(\"delta\").\r\n",
        "    mode(\"overwrite\").\r\n",
        "    save(externalTablePath)\r\n",
        "spark.sql(s\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '$externalTablePath'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// CREATE TABLE tableName USING DELTA PARTITIONED BY (...)\n",
        "\n",
        "df.write.\n",
        "  format(\"delta\").\n",
        "  mode(\"append\").\n",
        "  partitionBy(\"Col1\").\n",
        "  option(\"__partition_columns\", \"\"\"[\"Col1\"]\"\"\").\n",
        "  saveAsTable(\"PartitionedManagedDeltaTable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading from a storage path\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// SELECT * FROM delta.`/path/`\n",
        "\n",
        "spark.read.\n",
        "    format(\"delta\").\n",
        "    load(externalTablePath).\n",
        "    show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inserting from one table into another\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// INSERT INTO table1 SELECT * FROM table2\n",
        "\n",
        "spark.sql(\"SELECT * FROM ManagedDeltaTable\").\n",
        "    write.\n",
        "    format(\"delta\").\n",
        "    mode(\"append\").\n",
        "    save(externalTablePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// INSERT OVERWRITE table1 SELECT * FROM table2\n",
        "\n",
        "spark.sql(\"SELECT * FROM ManagedDeltaTable\").\n",
        "    write.\n",
        "    format(\"delta\").\n",
        "    mode(\"overwrite\").\n",
        "    save(externalTablePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Updating or deleting rows from a table\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// DELETE FROM tableName WHERE (...)\n",
        "\n",
        "import io.delta.tables._\n",
        "\n",
        "val dt = DeltaTable.forPath(externalTablePath)\n",
        "\n",
        "dt.delete(\"Col1 == 'foo'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// UPDATE tableName SET (...)\n",
        "\n",
        "val describeExtended = spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\")\n",
        "display(describeExtended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "// Get the path to the table in storage.\r\n",
        "val managedTablePath = describeExtended.\r\n",
        "    where(\"col_name == 'Location'\").\r\n",
        "    select(\"data_type\").\r\n",
        "    collectAsList().\r\n",
        "    get(0).\r\n",
        "    getString(0)\r\n",
        "\r\n",
        "// Construct the DeltaTable object from the path.\r\n",
        "val managedTable = DeltaTable.forPath(managedTablePath)\r\n",
        "\r\n",
        "// Run the update command.\r\n",
        "managedTable.update(\r\n",
        "        condition = expr(\"Col1 == 'foo'\"),\r\n",
        "        set = Map(\"Col2\" -> lit(\"foobar\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// UPDATE delta.`/path/` WHERE (...)\n",
        "DeltaTable.forPath(externalTablePath).\n",
        "    update(\n",
        "        condition = expr(\"Col1 == 'foo'\"),\n",
        "        set = Map(\"Col2\" -> lit(\"foobar\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merging two tables\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// MERGE INTO table1\n",
        "// USING table2\n",
        "// ON (...)\n",
        "// WHEN MATCHED THEN (...)\n",
        "// WHEN NOT MATCHED THEN (...)\n",
        "\n",
        "DeltaTable.forPath(externalTablePath).as(\"ExternalTable\").\n",
        "  merge(managedTable.as(\"ManagedTable\").toDF, \"ExternalTable.Col1 == ManagedTable.Col1\").\n",
        "  whenMatched.\n",
        "    update(Map(\"Col2\" -> lit(\"This row matched.\"))).\n",
        "  whenNotMatched.\n",
        "    insert(Map(\"Col2\" -> lit(\"This row did not match.\"))).\n",
        "  execute()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Changing the schema of a managed table.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// ALTER TABLE tableName ADD COLUMNS (...)\n",
        "// ALTER TABLE tableName CHANGE COLUMN (...)\n",
        "// ALTER TABLE tableName REPLACE COLUMNS (...)\n",
        "\n",
        "// Drop external table.\n",
        "spark.sql(\"DROP TABLE ExternalDeltaTable\")\n",
        "\n",
        "// Reconfigure the table using DataFrame APIs...\n",
        "\n",
        "// Recreate the table.\n",
        "df.write.\n",
        "    format(\"delta\").\n",
        "    mode(\"overwrite\").\n",
        "    save(externalTablePath)\n",
        "spark.sql(s\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '$externalTablePath'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuring table properties\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// ALTER TABLE delta.`/path`\n",
        "// SET TBLPROPERTIES(...)\n",
        "// TBLPROPERTIES(\n",
        "// delta.compatibility.symlinkFormatManifest.enabled=true)\n",
        "\n",
        "// No workaround available.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ SQL syntax: \n",
        "// \n",
        "// TBLPROPERTIES(delta.logRetentionDuration = \"interval <interval>\")\n",
        "// TBLPROPERTIES(delta.deletedFileRetentionDuration = \"interval <interval>\")\n",
        "\n",
        "// Can only set these globally.\n",
        "spark.conf.set(\"spark.databricks.delta.properties.defaults.delta.logRetentionDuration\", \"interval 2 days\")\n",
        "spark.conf.set(\"spark.databricks.delta.properties.defaults.delta.deletedFileRetentionDuration\", \"interval 1 days\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// SET spark.databricks.delta.commitInfo.userMetadata=”{custom metadata}” INSERT …\n",
        "\n",
        "// df.write.format(\"delta\")\n",
        "//   .mode(...)\n",
        "//   .option(\"userMetadata\", \"{custom metadata}\")\n",
        "//   .save(...)\n",
        "\n",
        "// No workaround available for these.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DeltaTable.forName()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Delta Lake 0.7+ syntax: \n",
        "// \n",
        "// DeltaTable.forName(tableName)\n",
        "\n",
        "val managedTablePath = spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").\n",
        "    where(\"col_name == 'Location'\").\n",
        "    select(\"data_type\").\n",
        "    collectAsList().\n",
        "    get(0).\n",
        "    getString(0)\n",
        "\n",
        "DeltaTable.forPath(managedTablePath)"
      ]
    }
  ]
}